{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure directory exists.\n",
      "On cuda:0\n"
     ]
    }
   ],
   "source": [
    "from cycada_utils import *\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'On {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. took batch norm out of D_T/D_S discriminators (paper supports this)\n",
    "2. Edit GANLoss \n",
    "\n",
    "<b> 3. Better separate GAN Loss between generator and discriminator </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "\n",
    "        self.D = []\n",
    "        self.L = []\n",
    "                \n",
    "        with open('/datacommons/carlsonlab/srs108/old/ol/Delhi_labeled.pkl', \"rb\") as fp:\n",
    "            for station in pkl.load(fp):\n",
    "#                 if station['PM25'] <600:\n",
    "                self.D.append(tuple((station['Image'][:,:,:3], station['PM25'])))\n",
    "                \n",
    "        with open('/datacommons/carlsonlab/srs108/old/ol/Lucknow.pkl', \"rb\") as fp:\n",
    "            for station in pkl.load(fp):\n",
    "                for datapoint in station:\n",
    "                    luck_img = datapoint['Image'][:,:,:3]\n",
    "                    if luck_img.shape == (224, 224,3):  \n",
    "                        self.L.append(tuple((luck_img, datapoint['PM'])))\n",
    "                        \n",
    "        self.L = random.choices(self.L, k= len(self.D))\n",
    "        \n",
    "    def __len__(self): return (len(self.D))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        transform  = transforms.Compose([\n",
    "                            transforms.ToPILImage(),\n",
    "                            transforms.RandomHorizontalFlip(),\n",
    "                            transforms.RandomVerticalFlip(),\n",
    "                            transforms.ToTensor()])\n",
    "\n",
    "        d_img = self.D[idx][0]\n",
    "        d_img = transform(d_img)        \n",
    "        l_img = self.L[idx][0]\n",
    "        l_img = transform(l_img)\n",
    "        \n",
    "        sample = {\n",
    "              'D img': d_img,\n",
    "              'D pm' : torch.tensor(self.D[idx][1]),\n",
    "              'L img': l_img,\n",
    "              'L pm' : torch.tensor(self.L[idx][1])\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = CyDataset()\n",
    "train, val = train_test_split(tr, test_size=0.2, random_state=42)\n",
    "train_dataloader = DataLoader(\n",
    "    train,\n",
    "    batch_size=5, # 1\n",
    "    shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val,\n",
    "    batch_size=1, # 1\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import torch.nn as nn\n",
    "\n",
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if norm_layer is not None:\n",
    "            if type(norm_layer) == functools.partial:\n",
    "                use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "            else:\n",
    "                use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = True\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                    nn.LeakyReLU(0.2, True)]\n",
    "\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult) if norm_layer is not None else nn.Identity(),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult) if norm_layer is not None else nn.Identity(),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "#         sequence += [nn.Softmax(dim=1)] \n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n",
    "\n",
    "        super(GANLoss, self).__init__()\n",
    "        \n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        \n",
    "        self.gan_mode = gan_mode\n",
    "        \n",
    "        if self.gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "            \n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            self.loss = None\n",
    "            \n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        \n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "            \n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        \n",
    "        return target_tensor.expand_as(prediction)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        \n",
    "        if self.gan_mode == 'vanilla':\n",
    "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "            loss = self.loss(prediction, target_tensor)\n",
    "            \n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            if target_is_real:\n",
    "                loss = -prediction.mean()\n",
    "            else:\n",
    "                loss = prediction.mean()\n",
    "                \n",
    "        return loss\n",
    "    \n",
    "#         # Calculate discriminator accuracy\n",
    "#         pred_labels = torch.round(torch.sigmoid(prediction)).squeeze()\n",
    "#         true_labels = torch.ones_like(pred_labels) if target_is_real else torch.zeros_like(pred_labels)\n",
    "        \n",
    "#         correct_predictions = torch.sum(pred_labels == true_labels).item()\n",
    "#         total_predictions = true_labels.numel()  \n",
    "        \n",
    "#         accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "#         return loss, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(out, target=None, onehot=True):\n",
    "    if target is None :\n",
    "        _, label = torch.max(out.data, 1)\n",
    "        return label\n",
    "    else:\n",
    "        if onehot:\n",
    "            _, label = torch.max(out.data, 1)\n",
    "        else: #if output is a one channel, set a label where threshold is 0.5\n",
    "            label = torch.where(out.data>torch.FloatTensor([0.5]), torch.ones(out.size()[0]).long(), torch.zeros(out.size()[0]).long())\n",
    "        acc = (label == target).sum().item() / target.size()[0]\n",
    "        return label, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gradient_penalty(netD, real_data, fake_data, device, type='mixed', constant=1.0, lambda_gp=10.0):\n",
    "    \"\"\"Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n",
    "\n",
    "    Arguments:\n",
    "        netD (network)              -- discriminator network\n",
    "        real_data (tensor array)    -- real images\n",
    "        fake_data (tensor array)    -- generated images from the generator\n",
    "        device (str)                -- GPU / CPU: from torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n",
    "        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n",
    "        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2\n",
    "        lambda_gp (float)           -- weight for this loss\n",
    "\n",
    "    Returns the gradient penalty loss\n",
    "    \"\"\"\n",
    "    if lambda_gp > 0.0:\n",
    "        if type == 'real':   # either use real images, fake images, or a linear interpolation of two.\n",
    "            interpolatesv = real_data\n",
    "        elif type == 'fake':\n",
    "            interpolatesv = fake_data\n",
    "        elif type == 'mixed':\n",
    "            alpha = torch.rand(real_data.shape[0], 1)\n",
    "            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(*real_data.shape)\n",
    "            alpha = alpha.to(device)\n",
    "            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "        else:\n",
    "            raise NotImplementedError('{} not implemented'.format(type))\n",
    "        interpolatesv.requires_grad_(True)\n",
    "        disc_interpolates = netD(interpolatesv)\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n",
    "                                        grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                                        create_graph=True, retain_graph=True, only_inputs=True)\n",
    "        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n",
    "        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp        # added eps\n",
    "        return gradient_penalty, gradients\n",
    "    else:\n",
    "        return 0.0, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "G_ST = ResnetGenerator(3, 3, 64, norm_layer=norm_layer, use_dropout=False, n_blocks=2)\n",
    "G_TS = ResnetGenerator(3, 3, 64, norm_layer=norm_layer, use_dropout=False, n_blocks=2)\n",
    "\n",
    "D_T = NLayerDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=None)\n",
    "\n",
    "D_S = NLayerDiscriminator(input_nc=3, ndf=64, n_layers=3, norm_layer=None)\n",
    "f_s = Multi_City_CNN()\n",
    "f_s.load_state_dict(torch.load('dlm4.pt'))\n",
    "\n",
    "G_ST.to(device)\n",
    "G_TS.to(device)\n",
    "D_S.to(device)\n",
    "D_T.to(device)\n",
    "f_s.to(device)\n",
    "\n",
    "\n",
    "ganloss = GANLoss(gan_mode='vanilla').to(device)       #use to fool discriminator\n",
    "cycleloss = torch.nn.L1Loss().to(device)               #difference between reconstructed img and original\n",
    "mseloss = torch.nn.MSELoss().to(device)       #difference between domain classifications between input img and generator output\n",
    "\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_ST.parameters(), G_TS.parameters()), lr=2e-4, betas=(0.5,0.999))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(D_S.parameters(), D_T.parameters()), lr=1e-5, betas=(0.5, 0.999))\n",
    "optimizer_f_s = torch.optim.Adam(f_s.parameters(), lr=1e-4, betas=(0.5, 0.999))    \n",
    "\n",
    "\n",
    "G_ST.apply(weights_init_normal)\n",
    "G_TS.apply(weights_init_normal)\n",
    "D_S.apply(weights_init_normal)\n",
    "D_T.apply(weights_init_normal)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Disc_Loss(netD, real, fake, gan_mode):\n",
    "    \"\"\"Calculate GAN loss for the discriminator\n",
    "\n",
    "    Parameters:\n",
    "        netD (network)      -- the discriminator D\n",
    "        real (tensor array) -- real images\n",
    "        fake (tensor array) -- images generated by a generator\n",
    "\n",
    "    Return the discriminator loss.\n",
    "    We also call loss_D.backward() to calculate the gradients.\n",
    "    \"\"\"\n",
    "    # Real\n",
    "    pred_real = netD(real)\n",
    "    loss_D_real = GANLoss(pred_real, True)\n",
    "    # Fake\n",
    "    pred_fake = netD(fake.detach())\n",
    "    loss_D_fake = GANLoss(pred_fake, False)\n",
    "    # Combined loss and calculate gradients\n",
    "\n",
    "    print(loss_D_real, loss_D_fake)\n",
    "    if gan_mode == \"wgangp\":\n",
    "        gradient_penalty, gradients = cal_gradient_penalty(netD,real,fake,device)\n",
    "        gradient_penalty.backward(retain_graph=True)\n",
    "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "    else:\n",
    "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "        \n",
    "    # Calculate discriminator accuracy\n",
    "    true_labels = torch.ones(real.size()[0]).long()\n",
    "    fake_labels = torch.zeros(fake.detach().size()[0]).long()\n",
    "    _, true_acc = prediction(pred_real.squeeze().cpu(), true_labels, onehot=False)\n",
    "    _, fake_acc = prediction(pred_fake.squeeze().cpu(), fake_labels, onehot=False)\n",
    "    acc = (true_acc + fake_acc) * 0.5\n",
    "    return loss_D, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(dataloader, e, i, save=False):\n",
    "\n",
    "    source = next(iter(dataloader))\n",
    "    G_ST.eval()\n",
    "    G_TS.eval()\n",
    "    real_source = source['D img'].type(Tensor) \n",
    "    fake_target = G_ST(real_source).detach()\n",
    "    real_target = source['L img'].type(Tensor)\n",
    "    real_lbl= source['D pm'].type(Tensor).float()\n",
    "    fake_source = G_TS(real_target).detach()\n",
    "    \n",
    "    recons = G_TS(fake_target).detach()\n",
    "    recont = G_ST(fake_source).detach() \n",
    "\n",
    "    real_S = make_grid(real_source, nrow=5, normalize=True, scale_each=True, padding=1)\n",
    "    fake_T = make_grid(fake_target, nrow=5, normalize=True, scale_each=True, padding=1)\n",
    "    reconS = make_grid(recons, nrow=5, normalize=True, scale_each=True, padding=1)\n",
    "    real_T = make_grid(real_target, nrow=5, normalize=True, scale_each=True, padding=1)\n",
    "    fake_S = make_grid(fake_source, nrow=5, normalize=True, scale_each=True, padding=1)\n",
    "    reconT = make_grid(recont, nrow=5, normalize=True, scale_each=True, padding=1)\n",
    "    # Arange images along y-axis    \n",
    "    image_grid = torch.cat((real_S, fake_T, real_T, fake_S, reconS, reconT), 2)\n",
    "    plt.imshow(image_grid.cpu().permute(1,2,0))\n",
    "    plt.title('Real Source vs Fake Target vs Recon Source | Real Target vs Fake Source vs Recon Target')\n",
    "    plt.axis('off')\n",
    "    plt.gcf().set_size_inches(10, 6)\n",
    "    if save:\n",
    "        plt.savefig(os.path.join('Figure_PDFs', f'epoch_{str(e+1)}_iter{str(i+1)}.png'), bbox_inches='tight', pad_inches=0, facecolor='white')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc741ea9e0fa42b2832f370ce6afdf1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss G: 2.6224093437194824\n",
      "GANLoss() GANLoss()\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'GANLoss' and 'GANLoss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-4d34cb8929d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0moptimizer_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mloss_real_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDisc_Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_S\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vanilla'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;31m#         loss_fake_s, _ = ganloss(D_S(fake_s.detach()), False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-e40e52e993b4>\u001b[0m in \u001b[0;36mDisc_Loss\u001b[0;34m(netD, real, fake, gan_mode)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_D_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_D_fake\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_D_real\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_D_fake\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Calculate discriminator accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'GANLoss' and 'GANLoss'"
     ]
    }
   ],
   "source": [
    "history = {'epoch':[],'G_loss': [], 'D_loss':[], 'cycloss':[],'semloss':[], 'batch':[]}\n",
    "\n",
    "num_classes = 2\n",
    "best_gen_loss = 1e6\n",
    "best_disc_loss = 1e6\n",
    "n_epochs = 2\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    for i, batch in tqdm(enumerate(train_dataloader)):\n",
    "        \n",
    "        real_S = batch['D img'].type(Tensor)\n",
    "        real_T = batch['L img'].type(Tensor)\n",
    "        real_lbl = batch['D pm'].type(Tensor).float()\n",
    "          \n",
    "        #Generator Forward Pass\n",
    "        G_ST.train()\n",
    "        G_TS.train()\n",
    "        \n",
    "        fake_t         = G_ST(real_S)             #source domain img, styled as target\n",
    "        fake_s         = G_TS(real_T)             #target domain img, styled as source\n",
    "        \n",
    "        recov_s        = G_TS(fake_t)\n",
    "        recov_t        = G_ST(fake_s)\n",
    "          \n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "# Run Regressor f_s  L_SEM = Ltask + Ltask;\n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "        set_requires_grad([f_s],requires_grad=False)\n",
    "        optimizer_f_s.zero_grad()\n",
    "    \n",
    "        #Run real img and stylistically transformed img through pretrained network, need 4 instances\n",
    "        pm_from_real_S  = f_s(real_S).squeeze(1)\n",
    "        pm_from_fake_T  = f_s(fake_t.detach()).squeeze(1)\n",
    "        \n",
    "        pm_from_real_T  = f_s(real_T).squeeze(1)\n",
    "        pm_from_fake_S  = f_s(fake_s.detach()).squeeze(1)\n",
    "\n",
    "        #compute MSE from PM values ***predicted*** from real image and fake image (same pm vals tho!!)\n",
    "        #DO NOT ASSESS WITH REAL PM LABEL, BUT WITH PREDICTION\n",
    "        \n",
    "        sem_loss_source    = mseloss(pm_from_real_S, pm_from_fake_T)  #Ltask(fs, G_TS(T), p(fs, T))                   \n",
    "        sem_loss_targ      = mseloss(pm_from_real_T,pm_from_fake_S)   #Ltask(fs, G_ST(S), p(fs, S))\n",
    "        sem_loss           = (sem_loss_source + sem_loss_targ)/2\n",
    "        \n",
    "#         sem_loss.backward()\n",
    "        optimizer_f_s.step()\n",
    "        \n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Train Generators G_ST and G_TS L_CYC;\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "        set_requires_grad([D_S, D_T],requires_grad=False)\n",
    "        set_requires_grad([G_ST, G_TS],requires_grad=True)\n",
    "        optimizer_G.zero_grad()\n",
    "    \n",
    "        #****train Generator to produce convincing results to fool D****\n",
    "        #extra ganloss run not included in objective......\n",
    "        \n",
    "        loss_GAN_S = ganloss(D_S(fake_t), True)  #L(D_S(G_ST(S)))             \n",
    "        loss_GAN_T= ganloss(D_T(fake_s), True)  #L(D_T(G_TS(T)))               \n",
    "        \n",
    "        # cycle loss (L1 Loss)      \n",
    "        loss_cycle_S   = cycleloss(recov_s, real_S)    # Lcyc(G_TS(G_ST(S)), S)\n",
    "        loss_cycle_T   = cycleloss(recov_t, real_T)    # Lcyc(G_ST(G_TS(T)), T)\n",
    "        loss_cycle     = loss_cycle_S + loss_cycle_T\n",
    "        \n",
    "        \n",
    "        loss_G = loss_GAN_S + loss_GAN_T + loss_cycle\n",
    "        \n",
    "        print(f'loss G: {loss_G}')       \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "# Train Discriminator S and Discriminator T L_GAN, L_GAN;\n",
    "# -------------------------------------------------------------------------------------------------------\n",
    "        set_requires_grad([D_S, D_T],requires_grad=True)\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        loss_real_s, _ = Disc_Loss(D_S, real_S, fake_s.detach(), 'vanilla')               \n",
    "#         loss_fake_s, _ = ganloss(D_S(fake_s.detach()), False)   \n",
    "        \n",
    "#         loss_D_S       = (loss_real_s + loss_fake_s)\n",
    "        \n",
    "#         loss_real_t, _ = ganloss(D_T(real_T), True)              \n",
    "#         loss_fake_t, _ = ganloss(D_T(fake_t.detach()), False)  \n",
    "        \n",
    "#         loss_D_T       = (loss_real_t + loss_fake_t)\n",
    "#         loss_D         = (loss_D_S + loss_D_T)\n",
    "\n",
    "#         print(f'loss D: {loss_D}')\n",
    "#         loss_D.backward()\n",
    "#         optimizer_D.step()\n",
    "        \n",
    "# # -----------------------------------------------------------------------------------------------------------\n",
    "# # Train Feature Dsicriminator D_ft L_GAN;\n",
    "# # -----------------------------------------------------------------------------------------------------------\n",
    "#         if loss_D < best_disc_loss:\n",
    "#             best_disc_loss = loss_D\n",
    "#             torch.save({ 'D_S': D_S.state_dict(),'D_T': D_T.state_dict()}, 'best_discs.pt')\n",
    " \n",
    "            \n",
    "#         if loss_G < best_gen_loss:\n",
    "#             best_gen_loss = loss_G\n",
    "#             torch.save({ 'G_ST': G_ST.state_dict(),'G_TS': G_TS.state_dict()}, 'best_gens.pt')\n",
    "# # ---------------------------------Visualization and Saving---------------------------------      \n",
    "        \n",
    "#         if (i+1) % 2500 == 0:\n",
    "#             with torch.no_grad():\n",
    "#                 sample_images(val_dataloader,e,i,save=False)\n",
    "#                 history['G_loss'].append(loss_G.item())\n",
    "#                 history['D_loss'].append(loss_D.item())\n",
    "#                 history['semloss'].append(sem_loss.item())\n",
    "#                 history['cycloss'].append(loss_cycle.item())\n",
    "#                 history['batch'].append(i+1)\n",
    "#                 history['epoch'].append(e+1)\n",
    "\n",
    "\n",
    "#     now = datetime.datetime.now()\n",
    "#     print(f\"Epoch {e + 1}/{n_epochs} finished at {now.time()}\\n\\\n",
    "#     [G Loss: {loss_G.item()}]\\t[D Loss: {loss_D.item()}]\\t[Cycle Loss: {loss_cycle.item()}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history)\n",
    "df.to_csv('history.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "generator_checkpoint = torch.load('/datacommons/carlsonlab/srs108/cicada/pixel_level/best_gens.pt')\n",
    "discriminator_checkpoint = torch.load('/datacommons/carlsonlab/srs108/cicada/pixel_level/best_discs.pt')\n",
    "\n",
    "#G's and D's will be pre loaded and set grad to false\n",
    "\n",
    "G_ST = ResnetGenerator(3, 3, 64, norm_layer=norm_layer, use_dropout=False, n_blocks=2)\n",
    "G_ST.load_state_dict(generator_checkpoint['G_ST'])\n",
    "\n",
    "G_TS = ResnetGenerator(3, 3, 64, norm_layer=norm_layer, use_dropout=False, n_blocks=2)\n",
    "G_TS.load_state_dict(generator_checkpoint['G_TS'])\n",
    "\n",
    "D_T = NLayerDiscriminator(3)\n",
    "D_T.load_state_dict(discriminator_checkpoint['D_T'])\n",
    "\n",
    "D_S = NLayerDiscriminator(3)\n",
    "D_S.load_state_dict(discriminator_checkpoint['D_S'])\n",
    "\n",
    "D_ft = FeatureDiscriminator()\n",
    "\n",
    "f_s = Multi_City_CNN()\n",
    "f_s.load_state_dict(torch.load('/datacommons/carlsonlab/srs108/cicada/pixel_level/saved_models/dlm4.pt'))\n",
    "\n",
    "f_t = LeNet(3)\n",
    "# f_t = Multi_City_CNN()\n",
    "# f_t.load_state_dict(torch.load('dlm4.pt'))\n",
    "\n",
    "G_ST.to(device)\n",
    "G_TS.to(device)\n",
    "D_S.to(device)\n",
    "D_T.to(device)\n",
    "D_ft.to(device)\n",
    "f_s.to(device)\n",
    "f_t.to(device)\n",
    "\n",
    "ganloss = GANLoss().to(device)                         #use to fool discriminator\n",
    "cycleloss = torch.nn.L1Loss().to(device)               #difference between reconstructed img and original\n",
    "ftloss = torch.nn.modules.CrossEntropyLoss().to(device)            #difference between generator output from input img and input img\n",
    "mseloss = torch.nn.MSELoss().to(device)       #difference between domain classifications between input img and generator output\n",
    "\n",
    "optimizer_G = torch.optim.Adam(itertools.chain(G_ST.parameters(), G_TS.parameters()), lr=2e-4, betas=(0.5,0.999))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(D_S.parameters(), D_T.parameters()), lr=1e-5, betas=(0.5, 0.999))\n",
    "optimizer_f_s = torch.optim.Adam(f_s.parameters(), lr=1e-4, betas=(0.5, 0.999))  \n",
    "\n",
    "optimizer_f_t = torch.optim.Adam(f_t.parameters(), lr=1e-5, betas=(0.5, 0.999))    #different learning rate??\n",
    "optimizer_D_ft = torch.optim.Adam(D_ft.parameters(), lr=1e-5, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "D_ft.apply(weights_init_normal)\n",
    "f_t.apply(weights_init_normal)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
